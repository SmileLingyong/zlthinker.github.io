<!DOCTYPE html>
<html lang="en">

	<head>
		<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1" />


	<title>Image Localization</title>


<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:title" content="Image Localization" />
<meta name="twitter:description" content="">

<meta name="description" content="">


	<meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o">


<link rel="icon" href="/assets/favicon.png">
<link rel="apple-touch-icon" href="/assets/touch-icon.png">
<link href="https://fonts.googleapis.com/css?family=Karla" rel="stylesheet">
<link rel="stylesheet" href="/assets/core.css">
<link rel="canonical" href="/image-localization">
<link rel="alternate" type="application/atom+xml" title="Lei Zhou's Blog" href="/feed.xml" />




<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


		<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
		<style>
			#taglist {
				color: gray;
				margin: 0;
				margin-top: 16px;
				padding: 0;
				list-style-type: none;
				text-align: center;
				vertical-align: middle;
			}
			#taglist li {
				display: inline;
			}
			#taglist li + li:before {
				content: ", ";
			}
		</style>

		<!-- Google Tag Manager -->
		<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
			new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
			j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
			'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
			})(window,document,'script','dataLayer','GTM-T5S7PZV');</script>
		<!-- End Google Tag Manager -->
	</head>

	<body>
		<!-- Google Tag Manager (noscript) -->
		<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5S7PZV"
			height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
		<!-- End Google Tag Manager (noscript) -->

		
<aside class="logo">

	

	<a href="/">
		<img src="/images/avatar.jpeg" class="gravatar">
	</a>
	<span class="logo-prompt">Back to Home</span>

</aside>


		<main>
			<noscript>
	<style>
		article .footnotes {
			display: block;
		}
	</style>
</noscript>

<article>
	<div class="center">
		<h1>Image Localization</h1>
		<time>March 16, 2018</time>
		<ul id="taglist">
			Tag:
			
		</ul>
	</div>

	<div class="divider"></div>

	<ul id="markdown-toc">
  <li><a href="#related-works" id="markdown-toc-related-works">Related works</a></li>
  <li><a href="#pnp-algorithm" id="markdown-toc-pnp-algorithm">PnP algorithm</a>    <ul>
      <li><a href="#p3p" id="markdown-toc-p3p">P3P</a></li>
      <li><a href="#p4p-p5p-pnp" id="markdown-toc-p4p-p5p-pnp">P4P, P5P, PnP</a></li>
      <li><a href="#epnp" id="markdown-toc-epnp"><a href="http://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf">EPnP</a></a></li>
      <li><a href="#other-pnps-and-insights" id="markdown-toc-other-pnps-and-insights">Other PnPs and insights</a></li>
    </ul>
  </li>
  <li><a href="#scene-coordinate-regression-network-score-net" id="markdown-toc-scene-coordinate-regression-network-score-net">Scene COordinate REgression Network (SCORE-Net)</a>    <ul>
      <li><a href="#network-architecture" id="markdown-toc-network-architecture">Network architecture</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h1 id="related-works">Related works</h1>

<ul>
  <li>Tutorial
    <ul>
      <li><a href="https://alexgkendall.com/media/presentations/lsvpr_2017_cvpr_tutorial_alex_kendall.pdf">Learning-based Visual Localization</a></li>
    </ul>
  </li>
  <li>Scene coordinate regression
    <ul>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2013/papers/Shotton_Scene_Coordinate_Regression_2013_CVPR_paper.pdf">Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></li>
      <li><a href="https://arxiv.org/pdf/1710.07965">Backtracking regression forests for accurate camera relocalization</a></li>
      <li><a href="http://cvlab-dresden.de/HTML/people/alexander_krull/publications/ICRA_2017.pdf">Random forests versus Neural Networks—What’s best for camera localization?</a></li>
      <li><strong>Sparse regression:</strong> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.pdf">DSAC-Differentiable RANSAC for camera localization</a></li>
      <li><strong>Dense regression:</strong> <a href="https://arxiv.org/pdf/1802.03237.pdf">Full-Frame Scene Coordinate Regression for Image-Based Localization</a>: The same idea as mine</li>
      <li><a href="https://arxiv.org/pdf/1805.08443.pdf">Scene Coordinate and Correspondence Learning for Image-Based Localization</a></li>
    </ul>
  </li>
  <li>Pose regression
    <ul>
      <li><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf?utm_source=mandiner&amp;utm_medium=link&amp;utm_campaign=mandiner_digit_201512">Posenet: A convolutional network for real-time 6-dof camera relocalization</a></li>
      <li><a href="https://arxiv.org/pdf/1509.05909.pdf">Modelling Uncertainty in Deep Learning for Camera Relocalization</a></li>
      <li><a href="https://arxiv.org/pdf/1611.07890.pdf">Image-based localization using LSTMs for structured feature correlation</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Kendall_Geometric_Loss_Functions_CVPR_2017_paper.pdf">Geometric loss functions for camera pose regression with deep learning</a></li>
    </ul>
  </li>
  <li>Temporal/RNN
    <ul>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Clark_VidLoc_A_Deep_CVPR_2017_paper.pdf">VidLoc: A deep spatio-temporal model for 6-DoF video-clip relocalization</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Deep_Feature_Flow_CVPR_2017_paper.pdf">Deep Feature Flow for Video Recognition</a></li>
      <li><a href="https://arxiv.org/pdf/1606.00487.pdf">Recurrent Fully Convolutional Networks for Video Segmentation</a></li>
      <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a></li>
    </ul>
  </li>
  <li>Traditional
    <ul>
      <li><a href="http://ieeexplore.ieee.org/abstract/document/7572201/">Efficient &amp; effective prioritized matching for large-scale image-based localization</a></li>
    </ul>
  </li>
  <li>Feature selection
    <ul>
      <li><a href="http://www.di.ens.fr/~josef/publications/knopp10.pdf">Avoiding confusing features in place recognition</a>: (ECCV 2010)</li>
      <li>feature re-weighting: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099829">Learned Contextual Feature Reweighting for Image Geo-Localization</a></li>
    </ul>
  </li>
  <li>Recent works
    <ul>
      <li><a href="https://arxiv.org/pdf/1712.03342.pdf">MapNet: Geometry-Aware Learning of Maps for Camera Localization</a>: Leverage the full map (CVPR2018, spotlight)</li>
      <li><a href="https://arxiv.org/pdf/1711.10228.pdf">Learning Less is More – 6D Camera Localization via 3D Surface Regression</a>: Align to surface</li>
      <li><a href="https://arxiv.org/pdf/1709.09905.pdf">X-View: Graph-Based Semantic Multi-View Localization</a>: Use segmentation</li>
      <li><a href="https://arxiv.org/pdf/1712.05773.pdf">Semantic Visual Localization</a>: Use segmentation (CVPR2018, ETH)</li>
      <li><a href="https://arxiv.org/pdf/1707.09092.pdf">Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions</a>: Outdoor benchmark (CVPR2018, spotlight, ETH)</li>
      <li><a href="">InLoc: Indoor Visual Localization with Dense Matching and View Synthesis</a>: (CVPR2018, spotlight, ETH)</li>
      <li>Pose regression + visual odometry <a href="https://arxiv.org/pdf/1803.03642.pdf">Deep Auxiliary Learning for Visual Localization and Odometry</a>: (ICLA2018)</li>
    </ul>
  </li>
  <li>encoder-decoder network
    <ul>
      <li>Scene coordinate regression: <a href="https://arxiv.org/pdf/1802.03237.pdf">Full-Frame Scene Coordinate Regression for Image-Based Localization</a></li>
      <li>Single-view depth estimation: <a href="https://arxiv.org/pdf/1704.07813.pdf">Unsupervised Learning of Depth and Ego-Motion from Video</a> <a href="https://github.com/tinghuiz/SfMLearner">Github</a></li>
      <li>Single-view depth estimation: <a href="https://arxiv.org/pdf/1709.06841.pdf">UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning</a></li>
      <li>Depth completion: <a href="http://deepcompletion.cs.princeton.edu/paper.pdf">Deep Depth Completion of a Single RGB-D Image</a></li>
      <li><strong>DispNet</strong>: <a href="https://arxiv.org/pdf/1512.02134.pdf">A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</a></li>
    </ul>
  </li>
  <li>geometry + deep learning
    <ul>
      <li><a href="https://gfx.uvic.ca/pubs/2018/zheng2018eigen/paper.pdf">Eigendecomposition-free Training of Deep Networks with Zero Eigenvalue-based Losses</a></li>
      <li><a href="https://gfx.uvic.ca/pubs/2018/yi2018learning/paper.pdf">Learning to Find Good Correspondences</a></li>
    </ul>
  </li>
  <li>Bayesian Neural Network &amp; MCMC Sampling
    <ul>
      <li><a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/HI8ta/bayesian-neural-networks">Coursera: Bayesian Neural Networks</a></li>
      <li>SGLD: <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">Bayesian Learning via Stochastic Gradient Langevin Dynamics</a>: (ICML2011)</li>
      <li>preconditioned SGLD: <a href="http://people.ee.duke.edu/~lcarin/aaai_psgld_final.pdf">Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks</a>: (AAAI 2016)</li>
      <li><a href="https://arxiv.org/pdf/1506.04416.pdf">Bayesian Dark Knowledge</a>: (NIPS2015)</li>
      <li><a href="https://arxiv.org/pdf/1509.05909.pdf">Modelling Uncertainty in Deep Learning for Camera Relocalization</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.pdf">DSAC-Differentiable RANSAC for camera localization</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Learning_Weight_Uncertainty_CVPR_2016_paper.pdf">Learning Weight Uncertainty with Stochastic Gradient MCMC for Shape Classification</a></li>
    </ul>
  </li>
  <li>Optical Flow
    <ul>
      <li><a href="https://arxiv.org/pdf/1504.06852.pdf">FlowNet</a></li>
      <li><a href="https://arxiv.org/pdf/1612.01925.pdf">FlowNet2.0</a> <a href="https://github.com/lmb-freiburg/flownet2">[github]</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.pdf">GeoNet</a> <a href="https://github.com/yzcjtr/GeoNet">[github]</a></li>
      <li><a href="https://arxiv.org/pdf/1607.07716.pdf">joint optical flow + segmentation</a></li>
    </ul>
  </li>
</ul>

<h1 id="pnp-algorithm">PnP algorithm</h1>

<blockquote>
  <p>“Perspective-n-Point (PnP) is the problem of estimating the pose of a calibrated camera given a set of n 3D points in the world and their corresponding 2D projections in the image.” from wikipedia</p>
</blockquote>

<h3 id="p3p">P3P</h3>

<p>P3P is the PnP problem in its minimal form. Let P be the camera center, and <script type="math/tex">A</script>, <script type="math/tex">B</script>, <script type="math/tex">C</script> be the 3D points in world frame. Let <script type="math/tex">\|PA\|=X</script>, <script type="math/tex">\|PB\|=Y</script>, <script type="math/tex">\|PC\|=Z</script>, <script type="math/tex">\alpha=\angle BPC</script>, <script type="math/tex">\beta=\angle APC</script>, <script type="math/tex">\gamma=\angle APB</script>, <script type="math/tex">p=2cos\alpha</script>, <script type="math/tex">q=2cos\beta</script>, <script type="math/tex">r=2cos\gamma</script>, <script type="math/tex">\|AB\| = c'</script>, <script type="math/tex">\|BC\| = a'</script>, <script type="math/tex">\|AC\| = b'</script>, as illustrated in Figure 1.</p>

<p><img src="/images/p3p.png" alt="P3P" />
<strong><center>Figure1. The P3P problem </center></strong></p>

<p>Then the P3P equation system is derived based on the law of cosines (余弦定理):</p>

<script type="math/tex; mode=display">\begin{cases} Y^2 + Z^2 - YZp - a'^2 = 0 \\ Z^2 + X^2 - XZq - b'^2 = 0  \\ X^2 + Y^2 - YXr - c'^2 = 0, \end{cases}</script>

<p>where <script type="math/tex">X</script>, <script type="math/tex">Y</script>, <script type="math/tex">Z</script> are variables to be determined.</p>

<p>To simply the equation system, we let <script type="math/tex">X= xZ</script>, <script type="math/tex">Y = yZ</script>, <script type="math/tex">c'= \sqrt{v}Z</script>, <script type="math/tex">a'=\sqrt{av}Z</script>, <script type="math/tex">b'=\sqrt{bv}Z</script>, then the equation system is turned into</p>

<script type="math/tex; mode=display">\begin{cases} y^2 + 1 - py - av = 0 \\ x^2 + 1 - qx -bv = 0 \\ x^2 + y^2 - xyr - v = 0, \end{cases}</script>

<p>where <script type="math/tex">x</script>, <script type="math/tex">y</script>, <script type="math/tex">v</script> are variables to be determined.</p>

<p>Eliminating <script type="math/tex">v = x^2 + y^2 - xyr</script>, we have</p>

<script type="math/tex; mode=display">\begin{cases} ax^2 + (a-1)y^2 - ar xy + py =1 \\ (b-1)x^2 + b y^2 - brxy + qx = 1.  \end{cases}</script>

<p>Essentially, the two quatratic equations define two conics, which may have infinite or at most four intersections. Therefore, <em>the P3P problem has either an infinite number of solutions or at most four physical solutions</em>.</p>

<h3 id="p4p-p5p-pnp">P4P, P5P, PnP</h3>

<p>Using more than 3 points provides redundant data for the solution of pose estimation. The linear algorithm is generally used [1]. The <script type="math/tex">n</script> points can form <script type="math/tex">\frac{n(n-1)}{2}</script> triangles and thus derive <script type="math/tex">\frac{n(n-1)}{2}</script> second degree polynomial equations based on 余弦定理. By eliminating the set of variables into a single one, we still have <script type="math/tex">\frac{n(n-1)}{2} - (n-1) = \frac{(n-1)(n-2)}{2}</script> fourth degree polynomial equations. When n = 5, we have the homogeneous equation system:</p>

<p><img src="/images/p5p.png" alt="P5P" /></p>

<p>The overdermined equation system can be easily solved by SVD decomposition, where <script type="math/tex">t_5</script> equals to the right singular vector corresponding to the smallest singular value. The solution can be generalized to cases when <script type="math/tex">n>5</script>.</p>

<p>When <script type="math/tex">n=4</script>, things are a little bit more complicated since the equation system above is underdetermined. We refer the readers to [1] for details.</p>

<h3 id="epnp"><a href="http://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf">EPnP</a></h3>

<p>EPnP algorithm is one of the most efficient PnP algorithm which has O(n) complexity. It is composed of three steps:</p>

<p>Step 1. In world coordinate system, find four control points: <script type="math/tex">c_j^w (j = 1,...,4)</script>. One of them is the centroid of the 3D points and the other three are vectors aligned with principle directions of the 3D point set. Then all the 3D points are represented by the <strong><a href="https://en.wikipedia.org/wiki/Barycentric_coordinate_system">barycentric coordinates</a></strong> as a linear combination of the four control points: <script type="math/tex">p_i^w = \sum_{j=1}^4 \alpha_{ij} c_j^w (i=1,...,n)</script>.</p>

<p>Step 2. In camera coordinate system, the equations between 3D points and control points still preserve:  <script type="math/tex">p_i^c = \sum_{j=1}^4 \alpha_{ij} c_j^c</script>. Given the 2D projections of the 3D points <script type="math/tex">\{p_i\}i=1,...,n</script>, we have <script type="math/tex">\omega_i \begin{bmatrix} u_i \\v_i \\ 1 \end{bmatrix} = K p_i^c = K \sum_{j=1}^4 \alpha_{ij} c_j^c</script>. Here, <script type="math/tex">\omega_i</script> and <script type="math/tex">\{c_j^c\}j=1,...,4</script> are unknowns. By filling in the camera intrinsic matrix, the equation can be expressed as follow:</p>

<script type="math/tex; mode=display">% <![CDATA[
\omega_i \begin{bmatrix} u_i \\ v_i \\ 1 \end{bmatrix} = \begin{bmatrix} f_u & 0 & u_c \\ 0 & f_v & v_c \\ 0 & 0 & 1 \end{bmatrix}  \begin{bmatrix} \sum_{j=1}^4 \alpha_{ij} x_j^c \\ \sum_{j=1}^4 \alpha_{ij} y_j^c \\ \sum_{j=1}^4 \alpha_{ij} z_j^c \end{bmatrix}. %]]></script>

<p>By eliminating <script type="math/tex">\omega_i = \sum_{j=1}^4 \alpha_{ij}z_j^c</script>, we get two linear equations:</p>

<script type="math/tex; mode=display">\begin{cases} \sum_{j=1}^4 \big( \alpha_{ij}f_u x_j^c + \alpha_{ij} (u_c - u_i) z_j^c \big) = 0 \\ \sum_{j=1}^4 \big( \alpha_{ij}f_v y_j^c + \alpha_{ij} (v_c - v_i) z_j^c \big) = 0 \end{cases}.</script>

<p>Therefore, each 3D-2D correspondence yields two linear equations with respect to a 12-vector <script type="math/tex">x=[x_1^c, y_1^c, z_1^c, ..., x_4^c, y_4^c, z_4^c]^T</script>. If consider all the correspondences, we generate a linear system of the form</p>

<script type="math/tex; mode=display">Mx = 0.</script>

<p>If we have normalized the coordinates of image pixels to <script type="math/tex">[u_i, v_i]^T</script>, the equation system becomes</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix} \alpha_{i1} & \alpha_{i2} & \alpha_{i3} & \alpha_{i4} & 0 & 0 & 0 & 0 & -u_i \alpha_{i1} & -u_i \alpha_{i2} & -u_i \alpha_{i3} & -u_i \alpha_{i4} \\ 0 & 0 & 0 & 0 & \alpha_{i1} & \alpha_{i2} & \alpha_{i3} & \alpha_{i4} & -v_i \alpha_{i1} & -v_i \alpha_{i2} & -v_i \alpha_{i3} & -v_i \alpha_{i4} \end{bmatrix} \begin{bmatrix} x_1^c \\ x_2^c \\ x_3^c \\ x_4^c \\ y_1^c \\ y_2^c \\ y_3^c \\ y_4^c \\ z_1^c \\ z_2^c \\ z_3^c \\ z_4^c \end{bmatrix} \\ =   \left(\begin{bmatrix} 1 & 0 & -u_i \\ 0 & 1 & -v_i \end{bmatrix}  \otimes \mathbf{\alpha}_i^T \right)  \mathbf{x} =  \mathbf{0}, %]]></script>

<p>where <script type="math/tex">\otimes</script> denotes Kronecker product.</p>

<p>Step 3. The solution of the linear equation system lies in the <strong>null space</strong> or <strong>kernel</strong> of <script type="math/tex">M^TM</script> and can be expressed as <script type="math/tex">x = \sum_{i=1}^N \beta_i v_i</script>, where <script type="math/tex">v_i</script> are the right-singular vectors of <script type="math/tex">M</script> corresponding to the zero singular values. Now the unknowns become <script type="math/tex">\{\beta_i\}i=1,...,N</script>. Based on the analysis in [2], the dimension <script type="math/tex">N</script> of null-space of <script type="math/tex">M^TM</script> varies from 1 to 4. The algorithm then considers all the four cases and choose the best set of <script type="math/tex">\{\beta_i\}i=1,...,N</script> with the miminum reprojection error.</p>

<h3 id="other-pnps-and-insights">Other PnPs and insights</h3>

<ul>
  <li>RPnP: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143946">A Robust O(n) Solution to the Perspective-n-Point Problem</a></li>
  <li>OPnP: <a href="http://www2.maths.lth.se/vision/publdb/reports/pdf/zheng-kuang-etal-iiccvi-13.pdf">Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></li>
  <li>REPPnP: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6909465">Very Fast Solution to the PnP Problem with Algebraic Outlier Rejection</a></li>
  <li>ASPnP: <a href="https://www.jstage.jst.go.jp/article/transinf/E96.D/7/E96.D_1525/_pdf">ASPnP: An Accurate and Scalable Solution to the Perspective-n-Point Problem</a></li>
  <li>The accuracy of solutions to the PnP problem is closely related to the configuration of the 3D point set [RPnP]. Ordinary 3D case, planar case, quasisingular case and “uncentered data” [EPnP].</li>
  <li>Under the assumption of independently and identically distribution (i.i.d.) Gaussian noise, minimizing the reprojection error is statistically optimal in the sense of maximum likelihood estimation (MLE).</li>
  <li>Pose ambiguity
    <ol>
      <li><a href="https://pdfs.semanticscholar.org/85ad/143e7fe0eafe536bb769de3cc5324500dd58.pdf">Robust Pose Estimation from a Planar Target</a></li>
    </ol>
  </li>
</ul>

<h1 id="scene-coordinate-regression-network-score-net">Scene COordinate REgression Network (SCORE-Net)</h1>

<h3 id="network-architecture">Network architecture</h3>

<ul>
  <li>
    <p><a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/">Globally and Locally Consistent Image Completion</a></p>
  </li>
  <li>
    <p>Dilated convolution (空洞卷积) Fully convolutional betwork, dilated conv</p>
  </li>
  <li>
    <p><strong><a href="http://statisticsbyjim.com/regression/heteroscedasticity-regression/">Heteroscedasticity</a></strong> 异方差，变量的方差各不相同。Remidial actions for severe heteroscedasticity are necessary.</p>
  </li>
  <li>
    <p><a href="https://stats.stackexchange.com/questions/97832/how-do-you-find-weights-for-weighted-least-squares-regression?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa">Weighted least square</a></p>
  </li>
  <li>
    <p><a href="http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20(Second%20Edition).pdf">“Data normalization (or called pre-conditioning in the numerical literature) is an essential step in the DLT algorithm.”</a> from Page 107</p>
  </li>
  <li>
    <p>rank-constrained optimization</p>
  </li>
  <li>
    <p>degenerate cases and the coplanar case</p>
  </li>
</ul>

<h1 id="reference">Reference</h1>
<p>[1] <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=784291">Linear N-Point Camera Pose Determination</a></p>

<p>[2] <a href="http://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf">EPnP: An Accurate O(n)Solution to the PnP Problem</a></p>



</article>

<div class="page-navigation">
	
    <a class="next" href="/deep-learning" title="NEXT: Deep Learning">&lt;&lt;</a>
		<span> &middot; </span>
  
		<a class="home" href="/" title="Back to Homepage">Home</a>
  
		<span> &middot; </span>
    <a class="prev" href="/random-forest" title="PREV: Random Forest">&gt;&gt;</a>
  
</div>

		</main>
                                                      

		<div class="footer">
<!--  <span class="block">Made with &hearts; using <a href="http://jekyllrb.com/">Jekyll</a> &amp; <a href="https://github.com/heiswayi/the-plain" title="Lei's Blog">The Plain</a> &middot; &lt;/&gt; on <a href="https://github.com/zlthinker" title="Hosted on GitHub">GitHub</a></span> -->
  <span class="block">&copy; 2018 Lei ZHOU</span>
</div>


		

			

		

		<!-- hit counter -->
		<br>
		<center>
		<img src="http://hitwebcounter.com/counter/counter.php?page=6818827&style=0024&nbdigits=5&type=ip&initCount=0" align="top" title="" Alt=""   border="1" height=18px/>    
		</center>  

	</body>

</html>
